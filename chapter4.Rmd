---
title: "chapter4"
author: "Veera Nenonen"
date: "25 marraskuuta 2018"
output: html_document
---

## Week 4: Clustering and classification

### Data

In this week's report we are focusing on clustering and classifying data. Clustering is a method for trying to find similar data points and dividing them in different groups. The groups are not known beforehand. In classification we are trying to do the same thing but we are actually knowing the groups. The data we are using for this is the *Boston* data that is part of the ``MASS`` package in R. The data is about housing values in Boston. Specific description of the data can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html). After loading the dataset and running the codes for structure and summary we'll get this kind of results: 

```{r, message = FALSE, warning = FALSE}
library(MASS)
library(corrplot)
library(ggplot2)

boston <- Boston

dim(boston)
str(boston)
summary(boston)
```

As we can see, the dataset has 14 variables and 506 observations. Most of the variables are numeric and two of them are integer type. These variables seem to be different features that can impact on the value of the housing. If we look at the summary, it looks like that the variables are distirbuted quite differently. For example, ``crim`` has minimun value < 1 and maximum value almost 90 while mean is less than 5 (3.677), and median is even less than 0.5. So apparently most of the values are located in the lower end. Instead, variable ``age`` seems to distribute much differently since its median is greater than 70 while minumum is 2.90 and maximum is 100.

To have a better understanding about the distributions of the variables and their relationships, we should make some graphs. Here are some histograms and bar charts made from the variables, and also a correlation matrix:

```{r, message = FALSE, warning = FALSE}
ggplot(data = boston, aes(boston$crim)) + geom_histogram()
ggplot(data = boston, aes(boston$zn)) + geom_histogram()
ggplot(data = boston, aes(boston$indus)) + geom_histogram()
ggplot(data = boston, aes(boston$chas)) + geom_bar() + scale_x_continuous(breaks = 0:1, labels=c("0","1"))
ggplot(data = boston, aes(boston$nox)) + geom_histogram()
ggplot(data = boston, aes(boston$rm)) + geom_histogram()
ggplot(data = boston, aes(boston$age)) + geom_histogram()
ggplot(data = boston, aes(boston$dis)) + geom_histogram()
ggplot(data = boston, aes(boston$rad)) + geom_bar()
ggplot(data = boston, aes(boston$tax)) + geom_histogram()
ggplot(data = boston, aes(boston$ptratio)) + geom_histogram()
ggplot(data = boston, aes(boston$black)) + geom_histogram()
ggplot(data = boston, aes(boston$lstat)) + geom_histogram()
ggplot(data = boston, aes(boston$medv)) + geom_histogram()


corrplot(cor(boston), method = "circle")
```

As we earlier noticed, the variables have really different distributions. Only one or two of them seem to distribute somehow normally (``rm`` and ``medv``). Now if we look at the correlation matrix, there are certainly multiple interesting relationships. The blue spots indicate positive correlation and red ones negative. For example variables ``tax`` and ``rad`` have correlation approximately 0.8, and variables ``nox`` and ``dis`` approximately -0.8. It means that if values of ``tax`` or ``rad`` increase, values of the other variable woud increase as well. And if values of ``nox`` or ``dis`` increase, values of the other variable would decrease. We do not know the causality between any of these variables - only linear correlation. 

Now that we are familiar with the data we can move on to some scaling.


### Scaling and standardization

Because these variables have really different ranges and distributions, it would be a good idea to scale them so that all of them are centered in a same way. The command ``scale()`` calculates the means and standard deviations of the variables and uses them to standardize them. This is what our data (named as ``boston_scaled``) looks like after scaling:

```{r}
boston_scaled <- as.data.frame(scale(boston))
summary(boston_scaled)
```

Now mean of all variables is 0. This way they are much more comparable since they are centered same way.

Next we'll make a new variable also called ``crime`` which is actually a categorical variable that is made from the original ``crime``. The new variable uses the quantiles from the original variable as its break points. After this, the old ``crime`` will be dropped and we'll use the new one.

```{r}
bins <- quantile(boston_scaled$crim)
bins

crime <- cut(boston_scaled$crim, breaks = bins , label = c("low", "med_low", "med_high", "high"), include.lowest = TRUE)
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)

boston_scaled <- data.frame(boston_scaled, crime)

```

This is all the data manipulation we will need! Now it's time for train and test sets.


### Test and train data sets

In machine learning it is important that we use different data for training the model and when testing its goodness. If this wasn't done, the model would probably overfit or not be suitable for other datasets in some other way. That is why we will split our current dataset. Usually the amount of data that is given for test set is around 10-40%, usually 20-30%. We will use 20% so that we have enough data for training:

```{r}
sample_size <- floor(0.80 * nrow(boston_scaled))
set.seed(42)
train_indices <- sample(seq_len(nrow(boston_scaled)), size = sample_size)

train <- boston_scaled[train_indices, ]
test <- boston_scaled[-train_indices, ]
```

After the splitting we are ready for some modelling.


### LDA

This time we are going to use linear discriminant analysis that is suitable for classification (so we know the groups before fitting the model!) tasks. Our categorical variable, which defines the class, is the one that we created in previous part. It means that it is going to be our target variable while the other variables will be the predictors. Let's fit the model and see what kind of results we are getting:

```{r}

lda.fit <- lda(crime ~., data = train)


lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)

plot(lda.fit, col = classes, pch = classes, dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```

From the model summary we can see the prior probabilites for each group. All groups seem to have almost same probability (because we used the quantiles as break points) so there is no dominant group. Then we can see some group means and coefficients for the redictors. LD1, LD2 and LD3 are the linear discriminants and because we have four groups we have three LDs. Proportion of trace tells how much the discriminant explains the variance between groups. The first one has clearly the highest explanation rate. 

The plot of the linear discriminants can show us how the groups are scattered and how the predictors impact on the model. The arrow that represents ``rad`` is significantly longer than others so it has the highest impact, and it is pointing to group *high*. The arrow of variable ``nox`` is also a bit longer than others. We can see that the group *high* is completely separated from others, though there are few lost *med-high* values.


### Validation

Now that we have trained our model, we need to test it to see how well it actually works. We'll use the model to the test data and check which group it chooses for each data point. Then we'll compare the results for the real values to see how good the model is for predicting the target variable ``crime``. If the model does not predict well, it needs to be tuned and then tested again. 

```{r}
correct_classes <- test$crime
test <- dplyr::select(test, -crime)

lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted =  lda.pred$class)

```

From the cross-tabulation we can see how many observations the model got right and how many it predicted to wrong classes. Apparently it did not predict any group completely right - though it got more than half of the points right. It did pretty good job with the group *high* because only two of the observations went wrong. This makes sense as we saw from the plot: the high group was separated from the other groups. The poorest predictions happened with the *med_low* since 42,8% of the observations were predicted wrong. So it seems that our model does not work that well with the middle groups. 

### K-means and grouping

After classification we try clustering. As mentioned earlier, now we do not know the groups in which we are trying to divide our data. We'll use the scaled ``boston`` data that is called in this case ``boston2``. After scaling we calculate the distances between the observations to know how they deviate:

```{r}
boston2 <- Boston
boston2_scaled <- scale(boston2)

boston2_dist <- dist(boston2_scaled)
summary(boston2_dist)
```

The summary shows the statistics of the distances. It seems that the deviation is quite much even though the data is scaled. 

For clustering we are going to use k-means algorithm that uses euclidean distances to group data points. After choosing the number of groups, the algorithm calculates centroids and divides data points into groups by the closest centroid. A tricky part is to choose optimal number of groups: too many groups means overfit and too little does not really give any information. We'll start with five groups:

```{r}
kms <- kmeans(boston2_scaled, centers = 5)
kms
```

The k-means statistics tells for example cluster sizes and means. Cluster sizes differ a lot, the largest one is almost five times bigger than the smallest one. This might indicate that we have too many clusters. Sum of squares is a rough estimate of the model goodness. Our percentage is 56.4 which is not very good. 

It would be good to try to optimize the number of groups that should be using in the custering. One way is to use sum of squares and especially the within cluster sum of squares. When there happens a sudden drop in that number we know that the optimal k is somewhere there. Let's try to plot that so that we can visualize what was just explained:

```{r}
k_max <- 10

twcss <- sapply(1:k_max, function(k){kmeans(boston2_scaled, k)$tot.withinss})

qplot(x = 1:k_max, y = twcss, geom = 'line')

```

Here we can see how the sum of squares change when used different number of clusters. It looks like the drop happens when k is two so we are going to use it this time.

Here are plotted all the variables with five clusters that we first used and then with two that was optimized:

```{r}
kms2 <- kmeans(boston2_scaled, centers = 2)

pairs(boston2_scaled, col = kms$cluster)
pairs(boston2_scaled, col = kms2$cluster)
```

As we can se from the scatter plots, more groups did not really give us more information. With some other data the optimal number of groups can be much different and there could be seeing logic within groups much easier. It could be also interesting to use less variables to see how that would impact to this algorithm. 